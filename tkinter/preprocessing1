# import libraries
import nltk
nltk.download('words')
nltk.download('stopwords')
import re
import string as st
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer # or LancasterStemmer, RegexpStemmer, SnowballStemmer
words = set(nltk.corpus.words.words())

def clean_text(text):
    
    def simplify_punctuation(text):
        text = str(text)
        text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # replace everything except letters/numbers
        return text
    
    def tokenize(text):
        text = str(text)
        text = re.split('\s+' ,text)
        return [x.lower() for x in text]
    
    def remove_stopwords(text):
        return [word for word in text if word not in nltk.corpus.stopwords.words('english')]
        
    def stemming(text):
        ps = PorterStemmer()
        return [ps.stem(word) for word in text]

    def lemmatize(text):
        word_net = WordNetLemmatizer()
        return [word_net.lemmatize(word) for word in text]

    text = simplify_punctuation(text) # remove punctuation
    text = tokenize(text) # tokenize
    text = remove_stopwords(text) # remove stopwords
    text = stemming(text) # stemming
    #text = lemmatize(text) # lemmatization, not used because words are stemmed. Could use in combination with stemming but didn't think it was necessary. 
    return text

## removing tokens to return to sentence
def return_sentences(tokens):
    return " ".join([word for word in tokens])
